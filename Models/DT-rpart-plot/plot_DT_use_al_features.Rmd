---
title: "Desicion Treewith rpart"
author: "Javad Khataei"
date: "8/27/2019"
output:
    html_document:
        code_folding : hide
        toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this markdown, we use `rpat` package to predict activity based on **X, Y, Z, Counts, metadata, and new features** with a decison tree and then we plot the tree.


## Read and prepare the data
```{r message=FALSE, warning=FALSE}

#import libraries
library(data.table)
library(tidyverse)
library(lubridate)
library(activityCounts)
library(magrittr)
library(scales)
library(caret)
library(stringr)
library(MLmetrics)
library(PRROC)
library(gridExtra)
library(kableExtra)
library(imputeTS)
library(Beap) # It's not on CRAN yet





# ---------------------------- Read the data ----------------------

# set the working directory to the folder that has the raw data and the intervals file
OS <- Sys.info()
if (OS["sysname"] == "Windows") {
  main_path <-
    "Z:/Research/dfuller/Walkabilly/studies/smarphone_accel/data"
} else {
  main_path <-
    "/Volumes/hkr-storage/Research/dfuller/Walkabilly/studies/smarphone_accel/data"
}

# Read the file with x,y,z axis and labels.
Ethica_df <-
  fread(paste0(main_path,"/Ethica_Jaeger_Merged/pocket/All_participants_pocket_with_count_for_Weka.csv"))


# ---------------------------- Prepare the data ----------------------------

# Participants 112 , 121 .122 , and 132 are not properly classified
working_df  <-
  Ethica_df %>% filter(!(participant_id %in% c(112, 121, 122, 132)))

# Model A_1 only uses raw accel data
working_df %<>%  dplyr::select(c(Counts_vec_mag, trimmed_activity))
working_df  %<>% filter(trimmed_activity != "transit")

# models canot work with strin
working_df$trimmed_activity  %<>% str_replace_all(" ", "_") %>%
  factor(
    levels = c(
      "Lying",
      "Sitting",
      "Self_Pace_walk",
      "Running_3_METs",
      "Running_5_METs",
      "Running_7_METs"
    ))


```

Note that we need to exclude Participants number 112 , 121 .122 , and 132 as they are not properly classified. Also, to save time, we run the models on  some of the data. Check the code for more details.



## Generate new features
We use the function ``GenerateFeatures` from the `Beap` package to create new features for `x_axis`, `y_axis`, and `z_axis`. Then we apply the models. The window is set to 2 seconds.


```{r message= T , warning=FALSE , eval = FALSE}
frequency <- 30
window_size_sec <- 2
# The number of features reduces by this
reduction_coef <-  window_size_sec *frequency 
new_features_df <- GenerateFeatures(raw_df = working_df, window_size_sec = window_size_sec, frequency = frequency)


# Increase the frequency
new_features_df <- new_features_df[rep(1:nrow(new_features_df) , each = reduction_coef), ]


# reduce the size of working_df to add new_features
working_df  %<>% slice(1:nrow(new_features_df))
## model A-4 uses new_features too
working_df <- bind_cols(working_df,new_features_df)
# some models cannpt train if there is any NA
working_df  %<>% na_interpolation(option =  "linear")


```




## Model with a DT with rpart and plot it
```{r message=TRUE, warning=FALSE}
# Load rpart and rpart.plot
library(rpart)
library(rpart.plot)

# Shrink, center, scale and then split the data
split_ratio <- 0.70
shrink <- 1
# Shrink
working_df %<>% dplyr::sample_frac(shrink)
message(paste0(shrink * 100, " % of the data will be used"))

# Split
training_indices <-
    createDataPartition(working_df$trimmed_activity,
                        p = split_ratio,
                        list = FALSE)
training_df <- working_df %>% dplyr::slice(training_indices)
testing_df <- working_df %>% dplyr::slice(-training_indices)

# # Scale and center
# preProcValues <-
#     preProcess(training_df, method = c("center", "scale"))
# training_df <- predict(preProcValues, training_df)
# testing_df <- predict(preProcValues, testing_df)
# message("Data is scaled and centered")

# Create a decision tree model
tree <- rpart(trimmed_activity ~ ., data = training_df, minsplit = 2, cp = .05) # cp = complexity parameter
# Visualize the decision tree with rpart.plot
rpart.plot(tree,
           box.palette = "RdBu",
           shadow.col = "gray",
           nn = F) # , cex = 0.50 

training_df_x <- training_df %>% select(-trimmed_activity)
training_df_y <- training_df %>% select(trimmed_activity)

testing_df_x <- testing_df %>% select(-trimmed_activity)
testing_df_y <- testing_df %>% select(trimmed_activity)

pred_testing_y <- predict(tree, newdata = testing_df_x, type = "class") %>% data.frame()

Accuracy(y_pred = pred_testing_y, y_true = testing_df_y)

library(maptree)
draw.tree(tree,cex=1)
library(rattle)
rattle::fancyRpartPlot(tree, cex = 0.65)

```

## Use C50 package to create a DT and plot it

```{r message=T, warning= T}
library(C50)
treeModel <- C5.0(training_df_x, training_df_y$trimmed_activity)
treeModel
summary(treeModel)
plot(treeModel,trial = 0)
#C50::C5imp(treeModel,pct = T) # feature importance
```

